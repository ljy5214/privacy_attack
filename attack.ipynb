{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    #定义一个数据集的抽象类\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        #获取长度\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #获取不同位置的元素\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        #使用tokenizer.encode_plus对文本进行分词和编码\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze() #输入文本的编码\n",
    "        attention_mask = encoding['attention_mask'].squeeze() #注意力掩码\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    # 类的初始化方法，接受一个参数num_classes，表示分类的类别数\n",
    "    def __init__(self, num_classes):\n",
    "        # 调用父类的初始化方法\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # 使用Hugging Face的transformers库加载预训练的bert-base-uncased模型\n",
    "        self.bert = BertModel.from_pretrained('./bert-base-uncased')\n",
    "        # 定义一个dropout层，丢弃概率为0.1\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # 定义一个全连接层，输入维度为bert模型的隐藏层大小，输出维度为分类的类别数\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    # 前向传播方法，接受inputs和attention_mask作为输入\n",
    "    def forward(self, inputs, attention_mask):\n",
    "        # 使用bert模型进行前向传播，返回的第一个元素为所有层的输出，第二个元素为池化后的输出\n",
    "        _, pooled_output = self.bert(inputs, attention_mask, return_dict=False)\n",
    "        # 对池化后的输出进行dropout操作\n",
    "        output = self.dropout(pooled_output)\n",
    "        # 将dropout后的输出输入全连接层，得到分类的logits\n",
    "        logits = self.fc(output)\n",
    "        # 返回logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些超参数\n",
    "max_length = 128 #最大长度\n",
    "batch_size = 32 #批大小\n",
    "num_epochs = 20 #迭代次数\n",
    "learning_rate = 1e-5 #学习率\n",
    "num_classes = 2  #分类数\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "task = 'race' #任务名称\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个预训练的 BERT 分词器（tokenizer）\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased')\n",
    "#数据处理\n",
    "train_dataframe = pd.read_csv('race/train10000.txt') #导入训练集\n",
    "train_dataframe[task] = train_dataframe[task].replace({'Positive': 1, 'Negative': 0}) #进行换词\n",
    "print(len(train_dataframe))\n",
    "\n",
    "test_dataframe = pd.read_csv('race/test2000.txt', encoding='ISO-8859-1') #导入测试集\n",
    "test_dataframe[task] = test_dataframe[task].replace({'Positive': 1, 'Negative': 0}) #进行换词\n",
    "print(len(test_dataframe))\n",
    "\n",
    "#从数据框中提取文本和标签，并将它们转换为列表\n",
    "train_texts = train_dataframe['text'].tolist()\n",
    "train_labels = train_dataframe[task].tolist()\n",
    "\n",
    "test_texts = test_dataframe['text'].tolist()\n",
    "test_labels = test_dataframe[task].tolist()\n",
    "\n",
    "# 生成相应数据集\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "\n",
    "# 加载相应数据集和模型\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化模型\n",
    "model = BertClassifier(num_classes) #在models.py中定义的模型，其中使用了bert模型\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #使用adamw优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() #设置模型为训练模式\n",
    "for epoch in range(num_epochs):\n",
    "    train_total_correct = 0\n",
    "    train_total_samples = 0\n",
    "\n",
    "    for input_ids, attention_mask, labels in train_dataloader:\n",
    "        #对每个批次的训练数据进行前向传播、计算损失、反向传播和参数更新\n",
    "\n",
    "        ## 将tensor移动到GPU上\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 优化器进行初始化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 生成模型的结果，这里会调用forward函数\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        # 通过argmax将模型输出转换为分类标签，并计算准确率\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "\n",
    "        # 计算分类准确率，并累加到总准确率中，并计算总样本数\n",
    "        train_total_correct += torch.sum(predicted_labels == labels).item()\n",
    "        train_total_samples += labels.size(0)\n",
    "\n",
    "        loss = criterion(outputs, labels) #计算损失函数\n",
    "\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step() #参数更新\n",
    "\n",
    "    train_accuracy = train_total_correct / train_total_samples\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #不计算梯度，对test集的元素进行验证\n",
    "        test_total_correct = 0\n",
    "        test_total_samples = 0\n",
    "\n",
    "        for test_input_ids, test_attention_mask, test_labels in test_dataloader:\n",
    "            test_input_ids = test_input_ids.to(device)\n",
    "            test_attention_mask = test_attention_mask.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            test_outputs = model(test_input_ids, test_attention_mask)\n",
    "            _, test_predicted_labels = torch.max(test_outputs, dim=1)\n",
    "\n",
    "            test_total_correct += torch.sum(test_predicted_labels ==\n",
    "                                            test_labels).item()\n",
    "            test_total_samples += test_labels.size(0)\n",
    "\n",
    "    test_accuracy = test_total_correct / test_total_samples\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Training Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert_classifier.pt') #保存模型参数至文件中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63f8def801cef14f9ccf5c2d60201e091c9c4b32021c524d30bf6934ac0b7a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
